# Use the official Ubuntu image as a base
FROM ubuntu:20.04

ARG DEBIAN_FRONTEND=noninteractive

# Set environment variables
ENV SPARK_VERSION=3.5.4
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$PATH
ENV SPARK_LOG4J_CONF=/opt/spark/conf/log4j2.properties
ENV PYSPARK_PYTHON=python3
ENV PYSPARK_DRIVER_PYTHON=python3

# Install dependencies
RUN apt-get update && apt-get install -y \
    wget \
    curl \
    vim \
    openjdk-11-jdk \
    python3 \
    python3-pip \
    && apt-get clean

# Download and install Apache Spark
RUN wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    && tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /opt \
    && mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} $SPARK_HOME \
    && rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz


RUN mkdir -p /shared-data
RUN chmod -R 755 ./shared-data

COPY /conf/log4j2.properties /opt/spark/conf

# Set the working directory
WORKDIR $SPARK_HOME

# Expose ports for Spark UI
EXPOSE 4040 8080 7077 6066

# Default command to run when starting the container
CMD ["spark-shell"]